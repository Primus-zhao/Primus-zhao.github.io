---
title: "Post: Show attend and tell"
date: 2017-09-09
categories:
  - Post
last_modified_at: 2017-09-09T13:34:25-05:00
---

最近在学习show and tell模型，这个模型主要有两个版本，这两我们首先分析Bengio团队提出的有attention model的版本。这个版本是2015年的论文中提出，在分析之后我们也会对image caption当前比较流行的模型进行介绍和分析。

### why attention? 
image captioning 的任务就是通过看图生成对应的描述。从整体角度来看，就是在辨识图中物体的同时再将这些物体之间的关系表述出来。当前的比较流行的思路是通过CNN来获得图中物体的辨识（隐式而非显式），获得了多物体的抽象信息后使用RNN模型来学习这些抽象信息之间的关系。对于单个图片来讲，CNN是肯定有能力将其中的物体信息表示出来的。根据CNN的特性，这些信息会保留在不同的channel当中，所以如何从这些channel当中得到所需的feature就成了关键，而这也是attention model能够登场的最重要原因。

### what's attention?
attention model从粗浅的意义上来将实际上就是从输入信息到选择权值的一个映射，由于可以通过神经网络来进行建模，所以可以与其他深度学习模型同步训练从而得到一个很好的结果。值得注意的是尽管原理简单，但是在具体应用的时候得到的权重和原模型的输入有多种不同的结合方式，其中带来的结果差异实际上值得细致分析。再进一步对attention model 进行细分的话可以分为hard和soft两种model,前者得出的权重具有one-hot的性质，即只会从原模型输入中取单一的channel，而后者则是所有输入的一个权重混合。这篇文章同时采取了两种方案并进行了对比，对比结果我们会在后边具体解释。

### how to attend?
话不多说，直接上公式
对于每个CNN channel输出的 \\(\{a_1, a_2,...,a_L\}\\), 我们可以采用如下的公式来计算每个channel对应的attention值：

$$e_ti=f_{att}(a_i, h_{t-1})$$

然后采用softmax方法：

$$\alpha_{ti} = \frac{exp(e_{ti})}{\sum_{k=1}^L{exp(e_{tk})}}$$

这样我们就得到了一组attention值\\(\alpha_{ti}\\), 之后我们通过如下的公式来得到新的输入向量：

$$\hat{z_t}=\phi(\{a_i\}, \{\alpha_i\})$$

在通过计算attention value的过程中我们可以看到，attention值完全是由本channel输入和前一状态的hidden state \\(h_{t-1}\\)来决定的，即由当前输入和历史信息共同来求解。
当然，在进行第一轮计算时是没有隐藏层信息的，论文里给出的方法是采用两个单独的前馈网络来处理，具体公式如下：

$$c_0=f_{init, c}(\frac{1}{L}\sum_{i}^L{a_i})$$

$$h_0=f_{init, h}(\frac{1}{L}\sum_{i}^L{a_i})$$

这种解决方案实际上是等价于在初始状态时之通过输入值而不考虑隐藏层状态来进行计算，其实与将隐藏层直接设为0意思上差不多，但是由于因为了新的网络，所以拟合能力应该强些， 在初始状态训练的过程中不会影响后续状态训练出的LSTM权值。
当然，这还不算完， 为了计算输出概率（其实就是计算应该是哪个词），最后又引入了一组全连接和幂函数，形式如下：

$$p(y_t|a, y_1^{t-1})\propto exp(L_o(Ey_{t-1}+L_hh_t+L_z\hat{z_t})))$$

其中的\\(L_o, L_h, L_z\\)通过训练得到。
文章之后进行了Hard和Soft attention的分析，为了保证思路的连贯性，我们先略过它们，最后再统一进行分析。
