# Hinton与Capsule计划（第一部分：诞生与崛起）
---
title: "Hinton与Capsule计划（第一部分：神经网络的诞生与崛起）"
date: 2017-09-25
categories:
  - Post
last_modified_at: 2017-09-25T21:00:25-00:00
---

几十年前，美国历史上最为（能）杰（撒）出（谎）的国务卿基辛格接受记者采访时曾被问及这样一个问题：“成绩出色的你为何没有选择留校任教？”基辛格给出了这样的回答：“学术圈的混乱情况使我向往中东地区国际关系的单纯。”虽有调侃的意味，却也道出了学术界派系争斗的残酷现实。而时下如火如荼的深度学习的发家史几乎可以说是这一情况最深刻的写照。为了能够全面的介绍Hinton及他宏大的Capsule计划，我们就有必要对深度学习（神经网络）的苦难发展史做一个简要的回顾。

## 潜龙勿用

神经网络的理论根基出现的非常之早。１９４９年Hebb提出了大名鼎鼎的Hebb　rule，随后在１９５７年Frank Rosenblatt基于这一理论设计出了感知器。从现代观点来看，这就是一个单层的最为简单的神经网络，同时其权值更新机制只靠单纯的叠加分类结果来实现。如此简单粗暴的方法自然不可能应对复杂的问题，对于非线性分类问题几乎可以说是无能为力。在故事情节推进之前我们还需要简单的介绍一下故事背景，尽管学术圈中有一众大牛都是做人工智能的，但是这些大牛之间可没有什么阶级情感。因为人工智能领域立山头的情况非常严重，根据其理论基础主要分为四大派系：１、连接主义（来源：神经科学，代表理论：神经网络），２、符号主义（来源：逻辑学，代表理论：专家系统），３、进化主义（来源：遗传生物学，代表理论：遗传算法），４、贝叶斯派（来源：统计学，代表理论：贝叶斯网络）。正如中国的各个门派之间要打个你死我活一样，这些派系之间也是常年的你来我往。而当时人工智能最为推崇的大牛是符号主义的马文明斯基。如果我们以神经网络为第一主角写篇小说，那这位老斯基无疑会成为最大的反面boss。当时的进化主义和贝叶斯派发展的还非常稚嫩，所以自然不会获得明斯基的关注，而偏巧神经网络由于具备模仿人类神经系统的特点，很容易让人把它和正统的人工智能结合起来。看到此情此景老爷子坐不住了，本着趁它虚要它命的精神，老爷子写了一本书，叫做《感知器》。不过不要被书名骗了，马老爷子可不是学雷锋帮忙推广感知器，他写这本书的感觉就好像是乔布斯心血来潮写了本《伟大的微软》或者马化腾搞了本《崛起的３６０》一样：书中强调指出了感知器的在解决XOR等非线性问题的拟合上的局限性，再加上老爷子本身的巨大影响力，使得这一招直中要害，几乎打得神经网络１０年没缓过神来。而Hinton正是在这个寒冰期攻读的神经网络的研究生，所以他曾在吴恩达的采访中追忆起当时的悲惨情景。有下面几幅图为证：

 <img src="./hinton1.jpg" width="400" height="400" alt="hinton追忆１"/>
 <img src="./hinton2.jpg" width="400" height="400" alt="hinton追忆２"/>
 <img src="./hinton3.jpg" width="400" height="400" alt="hinton追忆３"/>
 <img src="./hinton4.jpg" width="400" height="400" alt="hinton追忆４"/>

这种感觉就好像是现在的你花了大价钱跑到蓝翔去学习BP机修理专业一样，基本是毕业即宣告失业了。
好在当时的美国还有对神经网络理论抱有兴趣的团队，最终接纳了Hinton。而事实也证明天道有轮回，Hinton的这一坚持也带来了回报，其不懈的努力终于换来了带来神经网络第一次复兴的技术－－反向传播算法。

## 反向传播算法

正如罗杰彭罗斯在《皇帝的新脑》中曾经提到的一样，一本书中的每一个数学公式都会吓跑一半的读者，所以我这里尽量用概念性的描述来阐述反向传播的问题。不过，为了能够避免语言过于晦涩，还是会稍微涉及到一些小学数学公式及最简单的求导概念，如果你有困难的话可以来找我讨论。此外，对反向传播的语言阐述的简单和数学上实现其计算的困难会形成鲜明的对比。同组的老葛和我聊起这个事的时候还吐过槽：现在搞深度学习的，十个人里有九个不会写反向传播。玩笑归玩笑，其实我个人感觉这个比例甚至于还有点过于乐观了。
在开始正题之前，先要插播个广告。对于反向传播，目前看到所有的学术著作和网络短文及在线课程当中对其概念和原理阐述的最通俗易懂的当属Colah的博客[Colah's blog](http://colah.github.io/posts/2015-08-Backprop/)（这个Colah与Stanford的Andrew Kaparthy合称深度学习博客界的网红姐妹花，其对LSTM的介绍博客几乎被全天下的人引用）。为了尊重知识产权，这里只会简单的摘出其中的核心部分来讲述，如果想查看全文请点击原文链接。此外在征得了Colah大神本人的同意后我打算在另一篇博客将该文章全文汉化，一看英文就犯困的同学也可以考虑一下。接下来我们首先介绍计算图的概念

### 计算图

神经网络在进行计算的过程中并非是像普通意义上的数学一样公式化，本着一图胜千言的原则。神经网络在整个计算流程的表示是通过图模型来实现的，而这个代表了整个计算流程的图，简称计算图，一个简单的栗子如下：
假如我们想要做如下的一组计算：

$$c=a+b$$
$$b=d+1$$
$$e=c*d$$

那么通过计算图来表示这个计算流程的效果如下：
<div align="center">
<img src="./computation-graph.png" width="600" height="300" alt="计算图样例"/>
</div>
在计算图中，每个节点（node)代表一个变量及该变量的计算过程（算子），而连接节点的箭头代表了变量的输出方向，即哪些变量的计算需要该变量，比如指向c的a和b两个节点的箭头就代表了为了计算c必须获得a和b两个变量，这与我们刚才写出的\\(c=a+b\\)公式相对应。
构建好了计算图之后，我们就可以按照箭头流向进行计算了，计算过程中只需要记住一点：当指向一个node的所有node都已经求值后，就可以按照node中的算子对该node的值进行计算。在该计算图中，只需要按照顺序计算a,b->c->d->e即可，其中a,b为已知输入量。这里我们假设a,b分别为２和１，那么根据以上流程完成的计算图如下所示：
<div align="center">
<img src="./tree-eval.png" width="600" height="300" alt="计算图样例"/>
</div>

### 计算图的导数

在了解了计算图的基本概念之后，我们接下来分析各个元素相互之间的影响。从计算图可以看到，箭头起始的变量对箭头指向的变量是具有直接影响的，如a和b影响c,b直接影响d，那么a的变化会对c产生多大的影响呢？对这些变化的定量估计就需要用到本篇当中最难的一个数学概念了：偏导数。不过没有学过偏导也没关系，因为我们举的都是线性函数的例子，你只需要了解如下这个基本公式就好了：
某个变量a对b的偏导一般表示为
$$\frac{\partial a}{\partial b}$$
对于线性函数\\(y=c_0x\\)来说，我们有
$$\frac{\partial y}{\partial x} = c_0$$
而如果两个变量a和b不相关时，我们有
$$\frac{\partial a}{\partial b}=0$$

其偏导等于线性函数的参数。
对于我们的计算图，可以求出如下的结果：
$$\frac{\partial c}{\partial a}=\frac{\partial a}{\partial a} + \frac{\partial b}{\partial a}=1$$
类似的，对于e，我们有：
$$\frac{\partial e}{\partial c}=\frac{\partial (c*d)}{\partial c}=d$$

$$\frac{\partial e}{\partial d}=\frac{\partial (c*d)}{\partial d}=c$$

从整个计算图的角度来看，进行求导后的效果如下：
<div align="center">
<img src="./tree-eval-derivs.png" width="600" height="300" alt="计算图求导样例"/>
</div>

那么对于非直接连接的变量间的相互影响该如何估计？很简单，找到两个变量对应node的连接路径，将路径上的偏导乘个遍就可以了。例如在上图中如果我们想要计算\\(\frac{\partial c}{\partial a}\\)的话怎么办，只要沿从a到c的路径把所有偏导值都乘起来就ok。计算过程如下：

$$\frac{\partial e}{\partial a} = \frac{\partial e}{\partial c}*\frac{\partial c}{\partial a} = 2*1 = 2$$

So easy, huh? 但是如果我们要求的是\\(\frac{\partial e}{\partial b}\\)呢？这个时候我们就面临问题了，因为从b到e有两条路径，而这个偏导的正解是把两路的乘积相加，也就是：

$$\frac{\partial e}{\partial b}=\frac{\partial e}{\partial c}*\frac{\partial c}{\partial b} + \frac{\partial e}{\partial d}*\frac{\partial d}{\partial b} = 2*1+3*1=5$$

这也就完成了偏导的求解 
